@COMMENT This file was generated by bib2html.pl <https://sourceforge.net/projects/bib2html/> version 0.94
@COMMENT written by Patrick Riley <http://sourceforge.net/users/patstg/>
@COMMENT This file came from Jan Hendrik Metzen's publication pages at
@COMMENT http://www.informatik.uni-bremen.de/~jhm/
@article{fabisch_accounting_2015,
	title = {Accounting for {Task}-{Difficulty} in {Active} {Multi}-{Task} {Robot} {Control} {Learning}},
	issn = {0933-1875, 1610-1987},
	url = {http://link.springer.com/article/10.1007/s13218-015-0363-2},
	doi = {10.1007/s13218-015-0363-2},
	abstract = {Contextual policy search is a reinforcement learning approach for multi-task learning in the context of robot control learning. It can be used to learn versatilely applicable skills that generalize over a range of tasks specified by a context vector. In this work, we combine contextual policy search with ideas from active learning for selecting the task in which the next trial will be performed. Moreover, we use active training set selection for reducing detrimental effects of exploration in the sampling policy. A core challenge in this approach is that the distribution of the obtained rewards may not be directly comparable between different tasks. We propose the novel approach PUBSVE for estimating a reward baseline and investigate empirically on benchmark problems and simulated robotic tasks to which extent this method can remedy the issue of non-comparable reward.},
	language = {en},
	urldate = {2015-05-05},
	number = {"Advances in Autonomous Learning"},
        publisher={Springer Berlin Heidelberg},
	journal = {German Journal of Artificial Intelligence},
	author = {Fabisch, Alexander and Metzen, Jan Hendrik and Krell, Mario Michael and Kirchner, Frank},
	month = may,
	year = {2015},
	keywords = {Active learning, Artificial Intelligence (incl. Robotics), Contextual policy search, Multi-task learning, Software Engineering/Programming and Operating Systems},
	pages = {1--9},
        bib2html_pubtype = {Journal},
        bib2html_rescat = {Reinforcement Learning}
}
