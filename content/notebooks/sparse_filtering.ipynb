{
 "metadata": {
  "name": "",
  "signature": "sha256:436354607bb225a76fe3f5dec37c64186d3e6140b6a0d7fc2569248fac187e3a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Sparse Filtering\n",
      "================\n",
      "\n",
      "Unsupervised learning of features for images from the Olivetti faces dataset\n",
      "using the sparse filtering algorithm. This is based on the paper \"Sparse Filtering\"\n",
      "by the authors Jiquan Ngiam, Pang Wei Koh, Zhenghao Chen, Sonia Bhaskar, and Andrew Y. Ng published in NIPS 2011.\n",
      "\n",
      "This algorithm does not try to model the data's distribution but rather to learn features which\n",
      "are sparsely activated, in the sense that\n",
      " * for each example, only a small subset of features is activated (\"Population Sparsity\")\n",
      " * each feature is only activated on a small subset of he examples (\"Lifetime Sparsity\"\n",
      " * features are roughly activated equally often (\"High Dispersal\")\n",
      " \n",
      "This sparsity is encoded as an objective function and L-BFGS is used to minimize this\n",
      "function.\n",
      "\n",
      "This notebook illustrates the algorithm on the Olivetti faces dataset and compares it to Restricted Boltzmann Machines."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext watermark\n",
      "%watermark -a \"Jan Hendrik Metzen\" -d -v -m -p numpy,scikit-learn"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pylab as plt\n",
      "\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Install with \"pip install sparse_filtering\"\n",
      "from sparse_filtering import SparseFiltering"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Prepare dataset"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This will load the Olivetti faces dataset, normalize the examples (global and local centering), and convert each example into a 2D structure (64*64 pixel image). Thereupon, 25 patches of size 16x16pixels are extracted randomly from each image."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import fetch_olivetti_faces\n",
      "dataset = fetch_olivetti_faces(shuffle=True)\n",
      "faces = dataset.data\n",
      "\n",
      "n_samples, _ = faces.shape\n",
      "\n",
      "faces_centered = faces - faces.mean(axis=0)  # global centering\n",
      "\n",
      "faces_centered -= \\\n",
      "    faces_centered.mean(axis=1).reshape(n_samples, -1)  # local centering\n",
      "\n",
      "faces_centered = \\\n",
      "    faces_centered.reshape(n_samples, 64, 64)  # Reshaping to 64*64 pixel images\n",
      "    \n",
      "print(\"Dataset consists of %d faces\" % n_samples)\n",
      "_ = plt.imshow(faces_centered[0], cmap=plt.get_cmap('gray'))  # show one example image"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Extract 25 16x16 patches randomly from each image\n",
      "from sklearn.feature_extraction.image import extract_patches_2d\n",
      "patch_width = 16\n",
      "\n",
      "patches = [extract_patches_2d(faces_centered[i], (patch_width, patch_width),\n",
      "                              max_patches=25, random_state=i)\n",
      "              for i in range(n_samples)]\n",
      "patches = np.array(patches).reshape(-1, patch_width * patch_width)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Show 25 exemplary patches\n",
      "plt.figure(figsize=(8, 8))\n",
      "for i in range(25):\n",
      "    plt.subplot(5, 5, i+1)\n",
      "    plt.imshow(patches[i].reshape(patch_width, patch_width), cmap=plt.get_cmap('gray'))\n",
      "    plt.xticks([])\n",
      "    plt.yticks([])\n",
      "plt.tight_layout()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Train Sparse Filtering estimator"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Sparse Filtering estimator is trained on entires dataset and 64 features extractors are learned. Note that this is computationally expensive and might take several minutes. After training, the corresponding features are extracted for the whole dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_features = 64   # How many features are learned\n",
      "estimator = SparseFiltering(n_features=n_features, \n",
      "                            maxfun=200,  # The maximal number of evaluations of the objective function\n",
      "                            iprint=10)  # after how many function evaluations is information printed\n",
      "                                        # by L-BFGS. -1 for no information\n",
      "features = estimator.fit_transform(patches)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following graphic illustrates the learned feature detectors. Note that most of these correspond to Gabor-like edge detectors, while some seem to correspond to noise. The latter is potentially either because L-BFGS was stopped before convergence or because the number of features (64) was chosen too large."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure(figsize=(12, 10))\n",
      "for i in range(estimator.w_.shape[0]):\n",
      "    plt.subplot(int(np.sqrt(n_features)), int(np.sqrt(n_features)), i + 1)\n",
      "    plt.pcolor(estimator.w_[i].reshape(patch_width, patch_width),\n",
      "               cmap=plt.cm.RdYlGn, vmin=estimator.w_.min(),\n",
      "               vmax=estimator.w_.max())\n",
      "    plt.xticks(())\n",
      "    plt.yticks(())\n",
      "    plt.title(\"Feature %4d\" % i)\n",
      "plt.tight_layout()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Activation histogram\n",
      "============================\n",
      "\n",
      "We check if the learned feature extractors have the desired sparsity properties:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.hist(features.flat, bins=50)\n",
      "plt.xlabel(\"Activation\")\n",
      "plt.ylabel(\"Count\")\n",
      "_ = plt.title(\"Feature activation histogram\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The graphic confirms that features have small activation typically."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "activated_features = (features > 0.1).mean(1)\n",
      "plt.hist(activated_features)\n",
      "plt.xlabel(\"Feature activation ratio over all examples\")\n",
      "plt.ylabel(\"Count\")\n",
      "_ = plt.title(\"Lifetime Sparsity Histogram\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Lifetime Sparsity**: The figure confirms that each feature is only active (i.e., has an activation above 0.1) for a small ratio of the examples (approx. 25%)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "activated_features = (features > 0.1).mean(0)\n",
      "plt.hist(activated_features, bins=10)\n",
      "plt.xlabel(\"Ratio of active features in example\")\n",
      "plt.ylabel(\"Count\")\n",
      "_ = plt.title(\"Population Sparsity Histogram\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Population Sparsity**: Each example activates only few features (approx. 25-30% of the features)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.hist((features**2).mean(1))\n",
      "plt.xlabel(\"Mean Squared Feature Activation\")\n",
      "plt.ylabel(\"Count\")\n",
      "_ = plt.title(\"Dispersal Histogram\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**High Dispersal**: All features have similar statistics; no one feature has significantly more \u201cactivity\u201d than the other features. This is checked by plotting the mean squared activations of each feature obtained by averaging\n",
      "the squared values in the feature matrix across the examples."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Learn a Second Layer"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Learned a second layer of features using greedy layer-wise stacking on the features produced by the first layer."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_features2 = 10   # How many features are learned\n",
      "estimator2 = SparseFiltering(n_features=n_features2, \n",
      "                             maxfun=50,  # The maximal number of evaluations of the objective function\n",
      "                             iprint=10)  # after how many function evaluations is information printed\n",
      "                                        # by L-BFGS. -1 for no information\n",
      "features2 = estimator2.fit_transform(features.T)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure(figsize=(10, 10))\n",
      "for i in range(n_features2):\n",
      "    indices = np.argsort(estimator2.w_[i])[-10:][::-1]\n",
      "    #ind = np.where(np.abs(estimator2.w_[0]) > 1.0)[0]\n",
      "    for j, ind in enumerate(indices):\n",
      "        plt.subplot(10, n_features2, j*n_features2 + i + 1)\n",
      "        plt.pcolor(estimator.w_[ind].reshape(patch_width, patch_width),\n",
      "                   cmap=plt.cm.RdYlGn, vmin=estimator.w_.min(),\n",
      "                   vmax=estimator.w_.max())\n",
      "        plt.xticks(())\n",
      "        plt.yticks(())\n",
      "    #plt.title(\"Feature %2d\" % i)\n",
      "plt.tight_layout()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}